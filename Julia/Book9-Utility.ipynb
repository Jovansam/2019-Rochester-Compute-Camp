{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Optim, GR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do the utility example again. The Optim package does bracketed univariate optimization and  multivariate optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 3.;\n",
    "alpha_h = 1.5;\n",
    "assets = 0;\n",
    "ufun(h) = log(w*h + assets) + alpha_h*log(1 - h);\n",
    "ufun(0.5)\n",
    "hgrid = 0:0.01:0.99;\n",
    "plot(hgrid, ufun.(hgrid)) # Remember the dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Brent's Method\n",
       " * Search Interval: [0.200000, 0.800000]\n",
       " * Minimizer: 4.000000e-01\n",
       " * Minimum: 5.839169e-01\n",
       " * Iterations: 9\n",
       " * Convergence: max(|x - x_upper|, |x - x_lower|) <= 2*(1.5e-08*|x|+2.2e-16): true\n",
       " * Objective Function Calls: 10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = optimize(x -> -ufun(x), 0.2, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is as in Matlab, except for univariate problems it wants a bracket, not an initial guess. This is usually more efficient anyway. If you really need an initial guess version, you could make your function return a length 1 array. Alternatively, I have a package which does this.\n",
    "\n",
    "Here is the multivariate version for family labor supply. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Minimizer: [3.75e-01, 6.25e-02]\n",
       "    Minimum:   5.786697e-01\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     Nelder-Mead\n",
       "    Initial Point: [5.00e-01, 5.00e-01]\n",
       "\n",
       " * Convergence measures\n",
       "    √(Σ(yᵢ-ȳ)²)/n ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Iterations:    30\n",
       "    f(x) calls:    60\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = 3.;\n",
    "w2 = 2.;\n",
    "alpha_h = 1.5;\n",
    "assets = 0;\n",
    "function ufun(h)\n",
    "    h1, h2 = h[1], h[2]\n",
    "    (h1 >= 1 || h2 >= 1) && return -Inf\n",
    "    c = w1*h1 + w2*h2 + assets\n",
    "    c <= 0 && return -Inf\n",
    "    log(c) + alpha_h*log(1 - h1) + alpha_h*log(1 - h2)\n",
    "end\n",
    "ufun([0.5; 0.5])\n",
    "optimize(x -> -ufun(x), [0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Minimizer: [3.75e-01, 6.25e-02]\n",
       "    Minimum:   5.786697e-01\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     BFGS\n",
       "    Initial Point: [5.00e-01, 5.00e-01]\n",
       "\n",
       " * Convergence measures\n",
       "    |x - x'|               = 7.89e-07 ≰ 0.0e+00\n",
       "    |x - x'|/|x'|          = 2.10e-06 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|         = 9.73e-13 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|/|f(x')| = 1.68e-12 ≰ 0.0e+00\n",
       "    |g(x)|                 = 7.33e-11 ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Iterations:    6\n",
       "    f(x) calls:    19\n",
       "    ∇f(x) calls:   19\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(x -> -ufun(x), [0.5, 0.5], BFGS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above I changed the optimization algorithm from Nelder-Mead (slow, robust) to BFGS (fast, not as robust)\n",
    "\n",
    "Why did we need the return -Inf part? Try evaluating log of a negative number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "DomainError",
     "evalue": "DomainError with -2.0:\nlog will only return a complex result if called with a complex argument. Try log(Complex(x)).",
     "output_type": "error",
     "traceback": [
      "DomainError with -2.0:\nlog will only return a complex result if called with a complex argument. Try log(Complex(x)).",
      "",
      "Stacktrace:",
      " [1] throw_complex_domainerror(::Symbol, ::Float64) at ./math.jl:31",
      " [2] log(::Float64) at ./special/log.jl:285",
      " [3] log(::Int64) at ./special/log.jl:395",
      " [4] top-level scope at In[12]:1"
     ]
    }
   ],
   "source": [
    "log(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia is pickier than Matlab about types. Matlab will convert the output to complex automatically. This is usually not sensible and is actually horrible for performance (see Type Stability in the Julia documentation). \n",
    "\n",
    "Regardless of the language, you should always handle undefined utility values properly. Negative consumption or impossible hours is just -Inf utility. It is infinitely bad. If you do this, optimization algorithms will recognize this and retreat from any guesses which generate that outcome."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
